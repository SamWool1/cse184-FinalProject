{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#F00\">IMPORTANT</h1>\n",
    "Please read the README file before trying to run this .ipynb. This project relies on the library \"pypatent\" for scraping data, which can easily be installed by running \"pip install pypatent\".\n",
    "\n",
    "GitHub link: https://github.com/SamWool1/cse184-FinalProject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scraping and Cleaning Data</h1>\n",
    "Everything in this section does not need to be ran again. It was used in order to construct the dataset from which our visualizations were made, and this data has been saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generating A Sample</h2>\n",
    "In order to scrape our patents, we first needed to randomly generate a set of patent numbers which we would then scrape and use for our data wrangling. <strong>This step does not need to be run again, as the dataset has already been scraped and generated. It is assumed that a folder named \"samples\" already exists prior to running this code.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gen_sample\n",
    "for i in range(1980, 2019):\n",
    "    gen_sample.sampleByYear(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Scraping</h2>\n",
    "Once our samples were generated, we scraped our target website (http://patft.uspto.gov/) by constructing the URL for individual patents using the generated patent numbers. This scraped data became the dataset we used in order to construct our visualizations. <strong>This step does not need to be run again, as the dataset has already been scraped and generated. It is assumed that a folder named \"scrapes\" already exists prior to running this code, and that the patent numbers have already been generated.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrape_with_sample\n",
    "for year in range(1980, 2019):\n",
    "    scrape_with_sample.main(str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleaning</h2>\n",
    "Some data cleaning and minor wrangling was necessary before the data was ready for visualization. While cleaning for the racing bar chart for companies was handled within the function that generates the dataset for visualizing, additional cleaning and wrangling was used for creating a unified .csv file for the dataset for our other two racing bar charts. <strong>This step generates a single .csv file using the scraped data. It contains cleaned information pertaining to fields and locations for our scraped patents, and identifies the year of each patent as well.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrangle_data\n",
    "for year in range(1980, 2019):\n",
    "    print('Wrangling ' + str(year))\n",
    "    wrangle_data.main(str(year))\n",
    "print('All wrangling finished')\n",
    "\n",
    "\n",
    "import wrangle_loc_fields\n",
    "wrangle_loc_fields.createLocFieldSheet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Wrangling and Visualizations</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Wrangling</h2>\n",
    "We used Flourish and Datawrapper to create our visualizations, which required us to have specifically formatted .csv files. We used functions in wrangle_data.py to wrangle our data into formats that these websites would accept. <strong>These functions rely on the scraped dataset in the folder \"scrapes\", as well as the cleaned and unified dataset created in the \"Cleaning\" section.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wrangle_data\n",
    "# This should already be imported from the previous step\n",
    "\n",
    "wrangle_data.makeRacingBarCountries()\n",
    "wrangle_data.makeRacingBarFields()\n",
    "wrangle_data.getTimeDiff()\n",
    "wrangle_data.makeLineCSV()\n",
    "wrangle_data.makeRacingBar()\n",
    "wrangle_data.makeBarCumSum()\n",
    "wrangle_data.createUnifiedScrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualizations</h2>\n",
    "All of our visualizations are hosted at <a href=\"\"></a>. Please click the link to view them. You can also view them using the \"home.html\" file in the \"website\" directory on our GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
